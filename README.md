# EKSPAR ‚Äì Kamera-basiertes Personenz√§hlsystem

## üìå Projektziel

**EKSPAR** (Edge-KI-System zur Personenanalyse in R√§umen) ist ein datenschutzkonformes, modular aufgebautes Z√§hlsystem mit Echtzeitverarbeitung auf dem Raspberry Pi 5. Es z√§hlt anonym Personen, erkennt Eintrittsrichtungen und stellt die Ergebnisse √ºber ein Dashboard zur Verf√ºgung. Es werden **keine Bilder oder Videos gespeichert**. Ab Version 1.2 wird das System standardm√§√üig mit dem optimierten YOLOv11n-NCNN-Modell ausgeliefert.

## üõ† System√ºbersicht

* Raspberry Pi 5 + AI-Kamera (picamera2)
* Live-Objekterkennung mit YOLOv11n (NCNN-Inferenz auf ARM)
* Z√§hlung durch Eintritt in eine konfigurierbare Region (Bounding Box)
* Konfiguration der Eintrittsrichtung
* Metadatenspeicherung (JSON + SQLite)
* Visualisierung im Streamlit-Dashboard

## üìÇ Projektstruktur

```
EKSPAR/
‚îú‚îÄ‚îÄ ekspar.py                    # Hauptstarter (Dashboard + Z√§hlung)
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ detection/person_counter.py
‚îÇ   ‚îú‚îÄ‚îÄ config/bbox_config.json
‚îÇ   ‚îú‚îÄ‚îÄ config/direction_config.json
    ‚îú‚îÄ‚îÄ object_counter.py
‚îÇ   ‚îî‚îÄ‚îÄ camera/
‚îÇ       ‚îú‚îÄ‚îÄ capture_raw.py       # Einzelbild f√ºr Konfiguration
‚îÇ       ‚îî‚îÄ‚îÄ camera_interface.py  # Subprozess-Ausf√ºhrung f√ºr picamera2
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ dashboard.py             # Streamlit-Oberfl√§che
‚îÇ   ‚îî‚îÄ‚îÄ components.py            # UI-Komponenten
‚îú‚îÄ‚îÄ models/yolo11n.pt           # PyTorch-Modell (Legacy, optional)
‚îú‚îÄ‚îÄ models/yolo11n_ncnn_model/  # NCNN-Modell (Standard ab v1.2)
‚îú‚îÄ‚îÄ data/log.db                 # SQLite-Datenbank
‚îú‚îÄ‚îÄ data/counter.json           # Aktueller Z√§hlstand
‚îú‚îÄ‚îÄ static/last_config.jpg      # Konfigurationsbild
‚îú‚îÄ‚îÄ requirements.txt            # Python-Abh√§ngigkeiten
‚îî‚îÄ‚îÄ README.md                   
```

## üöÄ Schnellstart

### 1. Umgebung vorbereiten

```bash
sudo apt update && sudo apt install python3-picamera2 -y  # Kamera-Support
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2. System starten

```bash
python ekspar.py
```

Das Script erkennt automatisch, ob eine Konfiguration vorhanden ist, und startet:

* den **Z√§hlmodus** oder
* den **Konfigurationsmodus** (falls keine Bounding Box definiert ist)

## üîç Konfigurationsmodus

1. Starte `ekspar.py` ohne vorhandene `bbox_config.json`
2. Im Dashboard:

   * Schritt 1: Neues Bild aufnehmen (Kamera)
   * Schritt 2: Z√§hlbereich einzeichnen (Box)
   * Schritt 3: Eintrittsrichtung definieren (2 Pfeilpunkte)
   * Schritt 4: √úbersicht √ºberpr√ºfen und Konfiguration speichern
3. Danach beginnt automatisch der Z√§hlmodus

## üìä Live-Z√§hlung

* Die Kamera erfasst Personen im konfigurierten Bereich
* Die Bewegungsrichtung wird **nicht erkannt**, sondern anhand des konfigurierten Winkels umgedreht (z. B. `angle = 180` ‚Üí IN/OUT getauscht)
* Die **aktuelle Anzahl**, **IN/OUT-Zahlen** und **Verlauf** werden:

  * in `data/counter.json` gespeichert
  * in `data/log.db` (SQLite) geloggt
  * im Dashboard visualisiert

## üóì Dashboard-Funktionen

| Funktion               | Beschreibung                                                             |
| ---------------------- | ------------------------------------------------------------------------ |
| üë• Live-Z√§hler         | Echtzeit-Anzeige von IN, OUT, aktuelle Personenanzahl                    |
| üìä Verlauf             | Aggregierte Zeitreihe der Personen im Raum                               |
| üîπ Personen pro Stunde | Balkendiagramm der Eintritte ("Heute", "Gestern", "Letzte Woche")        |
| üî∏ Tagesverlauf (avg.) | Durchschnittlicher Tagesverlauf (z. B. 08:00, 09:00...) f√ºr mehrere Tage |
| üìÑ CSV-Export          | Zeitreihendaten als CSV-Datei exportieren                                |
| üì∑ Konfiguration       | Bildaufnahme, Bounding Box, Richtungspfeil                               |

## üõ† Hinweise zur Kamera

* Die Aufnahme erfolgt √ºber `picamera2` **au√üerhalb der virtuellen Umgebung**
* Dazu wird das Skript `capture_raw.py` via Subprozess aufgerufen
* Voraussetzung: Raspberry Pi OS mit `libcamera`

```bash
sudo apt install python3-picamera2
```

## üîé Technische Besonderheiten

* Kamera-Modussteuerung √ºber `camera.lock` ("config" vs. "counting")
* Headless-Betrieb m√∂glich (kein GUI erforderlich)
* Kein Cloud-Zugriff, volle Offline-Funktion

### üßê Modell-Inferenz: PyTorch vs. NCNN

Das System verwendet standardm√§√üig das **NCNN-optimierte YOLOv11n-Modell**, das speziell f√ºr ARM-Prozessoren (Raspberry Pi‚ÄØ5) optimiert ist. Es ersetzt das fr√ºhere PyTorch-Modell:

| Metrik         | NCNN       | PyTorch  |
| -------------- | ---------- | -------- |
| FPS            | 6.7        | 3.1      |
| Inference-Zeit | \~150‚ÄØms   | \~310‚ÄØms |
| Speicherbedarf | gering     | h√∂her    |
| Empfehlung     | ‚úÖ Standard | ‚ùå Legacy |

**Modellwechsel:**

```python
# PyTorch (alt)
MODEL_PATH = "models/yolo11n.pt"

# NCNN (neu)
MODEL_PATH = "models/yolo11n_ncnn_model"
```

Bei Bedarf kann das fr√ºhere Modell weiterhin verwendet werden, z. B. f√ºr Vergleiche oder Tests. Das Format `.pt` wird jedoch **nicht mehr empfohlen**.

Weitere Infos:

* NCNN-Integration: [ultralytics.com/integrations/ncnn](https://docs.ultralytics.com/integrations/ncnn/)
* Object-Counting-Architektur: [ultralytics.com/guides/object-counting](https://docs.ultralytics.com/guides/object-counting/)

## üß∞ Entwicklungsabh√§ngigkeiten

Siehe `requirements.txt`:

```
ultralytics==8.3.162
opencv-python==4.11.0.86
streamlit==1.44.1
streamlit-drawable-canvas==0.8.0
pillow==11.2.1
pandas==2.2.3
altair==5.5.0
```

---

> Entwickelt im Rahmen der Bachelorarbeit "Entwicklung eines Kamera-basierten
>
> Systems zur Personenz√§hlung und Analyse der Raumnutzung" am MCI
